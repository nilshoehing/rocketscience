{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "- Fill in the api keys that you are going to use and whether you are on Google Colab in the next cell and run it.\n",
        "- Then run all cells under the \"Setup for all models\" section and the \"Model Interface\" section\n",
        "- If you want to run CLIP models execute the cells in the \"CLIP models\" section, else skip to the \"VLMs\" section to run all other VLMs\n",
        "- In either section select the models you want to run\n",
        "- The \"Localisation Experiment\" section contains the code for section 5.1 in the paper, that tests gpt4o and o4-mini for their localisation capabilities"
      ],
      "metadata": {
        "id": "16PWGwTOcGvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL IN THOSE KEYS THAT YOU ARE GOING TO USE\n",
        "ANTHROPIC_KEY = \"\"        # Required for Claude Sonnet\n",
        "OPENAI_KEY = \"\"           # Required for gpt-4o and o4-mini\n",
        "OPENROUTER_KEY = \"\"       # Required for all other API models\n",
        "HUGGINGFACE_TOKEN = \"\"    # Required for Paligemma\n",
        "# ARE YOU ON GOOGLE COLAB?\n",
        "COLAB = True"
      ],
      "metadata": {
        "id": "gKUHCbNFcmaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uKTRrMr5Tjx"
      },
      "source": [
        "# Setup for all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYLvrRCSxrMk"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "!pip install open_clip_torch\n",
        "!pip install transformers\n",
        "!pip install --upgrade datasets\n",
        "!pip install openai\n",
        "!pip install anthropic\n",
        "!pip install gdown\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQJyuGm_uayf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import os\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import anthropic\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from urllib.request import urlopen\n",
        "import open_clip\n",
        "from open_clip import create_model_from_pretrained, get_tokenizer\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from abc import ABC, abstractmethod\n",
        "import gdown\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsLDaWvOoOZx"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"nilshoehing/rocketsciencebench\", split=\"train\")\n",
        "if HUGGINGFACE_TOKEN:\n",
        "  login(token=HUGGINGFACE_TOKEN)\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = \"/content/drive/MyDrive/RocketScienceResults/\"\n",
        "else:\n",
        "  PATH = \"RocketScienceResults/\"\n",
        "os.makedirs(PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEZsfzSqIrit"
      },
      "source": [
        "# Model-Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3Ux8zxy1mOE"
      },
      "outputs": [],
      "source": [
        "WINOGROUND_SCORES_FILE = PATH+\"vlm_winoground_scores.csv\"\n",
        "CATEGORY_SCORES_FILE = PATH+\"vlm_category_scores.csv\"\n",
        "\n",
        "def update_or_add_row(csv_filename, new_row, id_column='model_name'):\n",
        "    file_exists = os.path.isfile(csv_filename)\n",
        "\n",
        "    if file_exists:\n",
        "        with open(csv_filename, mode='r', newline='') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            rows = list(reader)\n",
        "            fieldnames = reader.fieldnames\n",
        "    else:\n",
        "        rows = []\n",
        "        fieldnames = list(new_row.keys())\n",
        "\n",
        "    row_exists = False\n",
        "    for index, row in enumerate(rows):\n",
        "        if row[id_column] == new_row[id_column]:\n",
        "            rows[index] = new_row\n",
        "            row_exists = True\n",
        "            break\n",
        "\n",
        "    if not row_exists:\n",
        "        rows.append(new_row)\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "def get_logfile_name(model_name):\n",
        "    model_base_name = model_name.split(\"/\")[-1]\n",
        "    return f\"{PATH}{model_base_name}_results.json\"\n",
        "\n",
        "def extract_single_quoted_strings(text):\n",
        "    return re.findall(r\"'(.*?)'\", text[1:-1])\n",
        "\n",
        "class Model(ABC):\n",
        "    \"\"\"Abstract base class for models.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, reasoning=False):\n",
        "        self.reasoning = reasoning\n",
        "        self.model_name = model_name\n",
        "        self.output_file = get_logfile_name(model_name)\n",
        "        if reasoning:\n",
        "            self.output_file = self.output_file.replace(\".json\", \"_reasoning.json\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, text, images) -> str:\n",
        "        \"\"\"Generate a response from the model given a prompt.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "      image = image.resize((1024, 1024))\n",
        "      buffered = BytesIO()\n",
        "      image.save(buffered, format=\"PNG\")\n",
        "      img_bytes = buffered.getvalue()\n",
        "      return base64.b64encode(img_bytes).decode(\"utf-8\")\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        # Start from existing state if exists\n",
        "        if os.path.exists(self.output_file):\n",
        "          with open(self.output_file, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "            starting_index = len(results)\n",
        "        else:\n",
        "          results = []\n",
        "          starting_index = 0\n",
        "\n",
        "        for i, item in enumerate(tqdm(dataset)):\n",
        "          if i < starting_index:\n",
        "            continue\n",
        "\n",
        "          image1 = self.preprocess_image(item[\"image1\"])\n",
        "          image2 = self.preprocess_image(item[\"image2\"])\n",
        "\n",
        "          if self.reasoning:\n",
        "            select_text_prompt = \"Which caption fits the image best? Reason about it and at the end write \\\"RESPONSE\\\" and reply only with the number 1 or 2. 1.) \" + item[\"text1\"] + \" 2.) \" + item[\"text2\"]\n",
        "            select_image_prompt1 = \"Which image fits the caption best? Reason about it and at the end write \\\"RESPONSE\\\" and reply only with the number 1 or 2. Caption: \" + item[\"text1\"]\n",
        "            select_image_prompt2 = \"Which image fits the caption best? Reason about it and at the end write \\\"RESPONSE\\\" and reply only with the number 1 or 2. Caption: \" + item[\"text2\"]\n",
        "          else:\n",
        "            select_text_prompt = \"Which caption fits the image best? Reply only with the number 1 or 2, nothing else. 1.) \" + item[\"text1\"] + \" 2.) \" + item[\"text2\"]\n",
        "            select_image_prompt1 = \"Which image fits the caption best? Reply only with the number 1 or 2, nothing else. Caption: \" + item[\"text1\"]\n",
        "            select_image_prompt2 = \"Which image fits the caption best? Reply only with the number 1 or 2, nothing else. Caption: \" + item[\"text2\"]\n",
        "\n",
        "          result_entry = {\n",
        "              \"id\": i,\n",
        "              \"select_text_prompt\": select_text_prompt,\n",
        "              \"select_image_prompt1\": select_image_prompt1,\n",
        "              \"select_image_prompt2\": select_image_prompt2,\n",
        "              \"response_1\": self.generate(select_text_prompt, [image1]),\n",
        "              \"response_2\": self.generate(select_text_prompt, [image2]),\n",
        "              \"response_3\": self.generate(select_image_prompt1, [image1, image2]),\n",
        "              \"response_4\": self.generate(select_image_prompt2, [image1, image2]),\n",
        "          }\n",
        "          results.append(result_entry)\n",
        "\n",
        "          # Save after each iteration to avoid losing progress\n",
        "          with open(self.output_file, \"w\") as f:\n",
        "              json.dump(results, f, indent=4)\n",
        "\n",
        "    def get_text_image_group_scores(self, result_file=WINOGROUND_SCORES_FILE):\n",
        "        with open(self.output_file, \"r\") as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "        text_correct_count = 0\n",
        "        image_correct_count = 0\n",
        "        group_correct_count = 0\n",
        "        for item in data:\n",
        "          if self.reasoning:\n",
        "              res1 = re.sub(r'\\D', '', item[\"response_1\"].split(\"RESPONSE\")[-1])\n",
        "              res2 = re.sub(r'\\D', '', item[\"response_2\"].split(\"RESPONSE\")[-1])\n",
        "              res3 = re.sub(r'\\D', '', item[\"response_3\"].split(\"RESPONSE\")[-1])\n",
        "              res4 = re.sub(r'\\D', '', item[\"response_4\"].split(\"RESPONSE\")[-1])\n",
        "          else:\n",
        "            res1 = re.sub(r'\\D', '', item[\"response_1\"])\n",
        "            res2 = re.sub(r'\\D', '', item[\"response_2\"])\n",
        "            res3 = re.sub(r'\\D', '', item[\"response_3\"])\n",
        "            res4 = re.sub(r'\\D', '', item[\"response_4\"])\n",
        "\n",
        "          text_correct_count += 1 if res1 == \"1\" and res2 == \"2\" else 0\n",
        "          image_correct_count += 1 if res3 == \"1\" and res4 == \"2\" else 0\n",
        "          group_correct_count += 1 if res1 == \"1\" and res2 == \"2\" and res3 == \"1\" and res4 == \"2\" else 0\n",
        "\n",
        "        denominator = len(data)\n",
        "        model = self.model_name if not self.reasoning else self.model_name + \"_reasoning\"\n",
        "        new_row = {'model_name': model, 'text_score': text_correct_count/denominator, 'image_score': image_correct_count/denominator, 'group_score': group_correct_count/denominator}\n",
        "        update_or_add_row(result_file, new_row)\n",
        "\n",
        "    def get_results_by_category(self, result_file=CATEGORY_SCORES_FILE):\n",
        "        with open(self.output_file, \"r\") as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "        keys = [\n",
        "            \"Horizontal Position\", \"Vertical Position\", \"Depth\",\n",
        "            \"Proximity\", \"Order\", \"Absolute Position\", \"Other\"\n",
        "        ]\n",
        "        results = {key: {\"text_correct_count\": 0, \"image_correct_count\": 0, \"group_correct_count\": 0} for key in keys}\n",
        "\n",
        "        categories = [extract_single_quoted_strings(d) for d in dataset[\"Category\"]]\n",
        "        for i, item in enumerate(data):\n",
        "          if self.reasoning:\n",
        "            res1 = re.sub(r'\\D', '', item[\"response_1\"].split(\"RESPONSE\")[-1])\n",
        "            res2 = re.sub(r'\\D', '', item[\"response_2\"].split(\"RESPONSE\")[-1])\n",
        "            res3 = re.sub(r'\\D', '', item[\"response_3\"].split(\"RESPONSE\")[-1])\n",
        "            res4 = re.sub(r'\\D', '', item[\"response_4\"].split(\"RESPONSE\")[-1])\n",
        "          else:\n",
        "            res1 = re.sub(r'\\D', '', item[\"response_1\"])\n",
        "            res2 = re.sub(r'\\D', '', item[\"response_2\"])\n",
        "            res3 = re.sub(r'\\D', '', item[\"response_3\"])\n",
        "            res4 = re.sub(r'\\D', '', item[\"response_4\"])\n",
        "\n",
        "          cats = categories[i]\n",
        "          for cat in cats:\n",
        "              results[cat][\"text_correct_count\"] += 1 if res1 == \"1\" and res2 == \"2\" else 0\n",
        "              results[cat][\"image_correct_count\"] += 1 if res3 == \"1\" and res4 == \"2\" else 0\n",
        "              results[cat][\"group_correct_count\"] += 1 if res1 == \"1\" and res2 == \"2\" and res3 == \"1\" and res4 == \"2\" else 0\n",
        "\n",
        "        model = self.model_name if not self.reasoning else self.model_name + \"_reasoning\"\n",
        "        new_row = {'model_name': model}\n",
        "\n",
        "        flattened_categories = [item for s in categories for item in s]\n",
        "        c = Counter(flattened_categories)\n",
        "        for key, value in results.items():\n",
        "            new_row[f\"{key}_ts\"] = value[\"text_correct_count\"] / c[key]\n",
        "            new_row[f\"{key}_is\"] = value[\"image_correct_count\"] / c[key]\n",
        "            new_row[f\"{key}_gs\"] = value[\"group_correct_count\"] / c[key]\n",
        "        print(result_file)\n",
        "        update_or_add_row(result_file, new_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk_UfmrqwOTg"
      },
      "source": [
        "# CLIP models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SId933syhLSp"
      },
      "outputs": [],
      "source": [
        "class OpenCLIPModel(Model):\n",
        "    def __init__(self, model_name, model_path):\n",
        "        super().__init__(model_name+model_path.split(\"/\")[-1])\n",
        "        if model_path.split(\"/\")[-1] == \"negCLIP.pt\":\n",
        "          gdown.download(\"https://drive.google.com/uc?id=1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ&confirm=t\", quiet=False)\n",
        "\n",
        "        self.model, self.preprocess = create_model_from_pretrained(model_name, pretrained=model_path,device=device, load_weights_only=False)\n",
        "        self.tokenizer = get_tokenizer(model_name)\n",
        "        print(model_name, model_path)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate(self, text, images) -> str:\n",
        "        pass # not used for CLIP models\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        results = []\n",
        "        for i, item in enumerate(tqdm(dataset)):\n",
        "\n",
        "          image = torch.stack([self.preprocess(item[\"image1\"].convert(\"RGB\")), self.preprocess(item[\"image2\"].convert(\"RGB\"))]).to(device)\n",
        "          text = self.tokenizer([item[\"text1\"], item[\"text2\"]], context_length=self.model.context_length).to(device)\n",
        "\n",
        "          with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            image_features = self.model.encode_image(image)\n",
        "            text_features = self.model.encode_text(text)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            image_probs = (100.0 * text_features @ image_features.T) # images (columns), texts (rows)\n",
        "\n",
        "          select_text_prompt = \"\"\n",
        "          select_image_prompt1 = item[\"text1\"]\n",
        "          select_image_prompt2 = item[\"text2\"]\n",
        "\n",
        "          result_entry = {\n",
        "              \"id\": i,\n",
        "              \"select_text_prompt\": select_text_prompt,\n",
        "              \"select_image_prompt1\": select_image_prompt1,\n",
        "              \"select_image_prompt2\": select_image_prompt2,\n",
        "              \"response_1\": \"1\" if image_probs[0][0].item() > image_probs[1][0].item() else \"2\",\n",
        "              \"response_2\": \"2\" if image_probs[1][1].item() > image_probs[0][1].item() else \"1\",\n",
        "              \"response_3\": \"1\" if image_probs[0][0].item() > image_probs[0][1].item() else \"2\",\n",
        "              \"response_4\": \"2\" if image_probs[1][1].item() > image_probs[1][0].item() else \"1\",\n",
        "          }\n",
        "          results.append(result_entry)\n",
        "\n",
        "          # Save after each iteration to avoid losing progress\n",
        "          with open(self.output_file, \"w\") as f:\n",
        "              json.dump(results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows a list of available models\n",
        "open_clip.list_pretrained()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VAPuQrtOf3KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T5pnP5o6BNN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Add all models to the list, each model is identified by a pair of strings\n",
        "models = [('EVA02-B-16', 'merged2b_s8b_b131k'),('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
        " ('ViT-B-16-SigLIP','webli'),('ViT-L-16-SigLIP-384', 'webli'),('ViT-L-14-CLIPA', 'datacomp1b'),\n",
        "  ('ViT-L-16-SigLIP2-512', 'webli'),('coca_ViT-B-32', 'laion2b_s13b_b90k'),('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
        "   ('ViT-B-16-SigLIP2-512', 'webli'),('ViT-B-16', 'openai'),('ViT-B-32', 'openai')]\n",
        "\n",
        "# model.evaluate() produces json logs of the responses\n",
        "# model.get_results_by_category() and model.get_text_image_group_scores() produce csv tables with scores\n",
        "# all files will be placed in the RocketScienceResults folder\n",
        "for model_name, model_path in models:\n",
        "  openclip_model = OpenCLIPModel(model_name, model_path)\n",
        "  openclip_model.evaluate(dataset)\n",
        "  openclip_model.get_results_by_category()\n",
        "  openclip_model.get_text_image_group_scores()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxqyuSplwUmF"
      },
      "source": [
        "# VLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5soQ2sSIozA"
      },
      "outputs": [],
      "source": [
        "class OpenAIModel(Model):\n",
        "    def __init__(self, api_key: str, model_name: str = \"gpt-4o\", reasoning=False, base_url =\"https://api.openai.com/v1\"):\n",
        "        super().__init__(model_name, reasoning = reasoning)\n",
        "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "\n",
        "    def generate(self, text, images) -> str:\n",
        "      delay = 1\n",
        "      for i in range(6):\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[\n",
        "              {\"role\": \"user\", \"content\": [\n",
        "                  {\"type\": \"text\", \"text\": text},\n",
        "                  *[{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"}} for image in images]\n",
        "              ]}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "            break\n",
        "        time.sleep(delay * (2 ** i))  # Exponential backoff\n",
        "      return None\n",
        "\n",
        "class OpenAIReasoningModel(Model):\n",
        "    def __init__(self, api_key: str, model_name: str = \"o4-mini\", base_url =\"https://api.openai.com/v1\"):\n",
        "        super().__init__(model_name)\n",
        "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "\n",
        "    def generate(self, text, images) -> str:\n",
        "      for i in range(6):\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[\n",
        "              {\"role\": \"user\", \"content\": [\n",
        "                  {\"type\": \"text\", \"text\": text},\n",
        "                  *[{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"}} for image in images]\n",
        "              ]}\n",
        "            ],\n",
        "            reasoning_effort=\"medium\",\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "            break\n",
        "        time.sleep(delay * (2 ** i))  # Exponential backoff\n",
        "      return None\n",
        "\n",
        "class AnthropicModel(Model):\n",
        "    def __init__(self, api_key: str, model_name: str = \"claude-3-7-sonnet-20250219\"):\n",
        "        super().__init__(model_name)\n",
        "        self.client = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "    def generate(self, text, images) -> str:\n",
        "      message = self.client.messages.create(\n",
        "        model=self.model_name,\n",
        "        max_tokens=50,\n",
        "        temperature=0.0,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": text\n",
        "                    },\n",
        "                    *[{\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\",\"data\": image}} for image in images]\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "      )\n",
        "      return message.content[0].text\n",
        "\n",
        "class PaliGemma(Model):\n",
        "    def __init__(self, model_name: str):\n",
        "        super().__init__(model_name.split(\"/\")[-1])\n",
        "        self.model = PaliGemmaForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.model = self.model.eval().to(device)\n",
        "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        return image\n",
        "\n",
        "    def generate(self, text, images) -> str:\n",
        "        inputs = self.processor(images=[images], text=\"answer en \" + text + \"\\n\", return_tensors=\"pt\").to(device)\n",
        "        output = self.model.generate(**inputs, max_new_tokens=20, top_k=1)\n",
        "        res = self.processor.decode(output[0], skip_special_tokens=True)\n",
        "        return res.split(\"\\n\")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AVAILABLE MODELS\n",
        "# VLMs\n",
        "# model = AnthropicModel(model_name=\"claude-3-7-sonnet-20250219\", api_key=ANTHROPIC_KEY)\n",
        "# model = OpenAIModel(model_name = \"gpt-4o\", api_key=OPENAI_KEY)\n",
        "# model = OpenAIModel(model_name = \"qwen/qwen-2.5-vl-72b-instruct\", api_key=OPENROUTER_KEY, base_url=\"https://openrouter.ai/api/v1\")\n",
        "# model = OpenAIModel(model_name = \"qwen/qwen-vl-max\", api_key=OPENROUTER_KEY, base_url=\"https://openrouter.ai/api/v1\")\n",
        "# model = OpenAIModel(model_name = \"meta-llama/llama-4-maverick\", api_key=OPENROUTER_KEY, base_url=\"https://openrouter.ai/api/v1\")\n",
        "\n",
        "# # VLMs with explicit chain of thought\n",
        "# model = OpenAIModel(model_name = \"gpt-4o\", reasoning=True, api_key=OPENAI_KEY)\n",
        "# model = OpenAIModel(model_name = \"meta-llama/llama-4-maverick\", reasoning=True, api_key=OPENROUTER_KEY, base_url=\"https://openrouter.ai/api/v1\")\n",
        "\n",
        "# # VLMs with implicit chain of thought\n",
        "# model = OpenAIReasoningModel(model_name=\"o4-mini\",api_key=OPENAI_KEY)\n",
        "# model = OpenAIModel(model_name = \"google/gemini-2.5-pro-preview-03-25\", api_key=OPENROUTER_KEY, base_url=\"https://openrouter.ai/api/v1\")\n",
        "\n",
        "# Paligemma, works on a Colab T4-Gpu\n",
        "# model.evaluate() produces json logs of the responses\n",
        "# model.get_results_by_category() and model.get_text_image_group_scores() produce csv tables with scores\n",
        "# all files will be placed in the RocketScienceResults folder\n",
        "model = PaliGemma(\"google/paligemma-3b-mix-448\")\n",
        "model.evaluate(dataset)\n",
        "model.get_text_image_group_scores()\n",
        "model.get_results_by_category()"
      ],
      "metadata": {
        "id": "F9d6-O0giXEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "158FrJhstc7G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-Kfv-k5ultd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Localisation Experiment"
      ],
      "metadata": {
        "id": "eNXG-8yc9MDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objects = \"\"\"{'left': 'black sock', 'right': 'metal bottle'}\n",
        "{'left': '3 red berries', 'right': '1 glass'}\n",
        "{'left': 'five red berries', 'right': 'six brussel sprouts'}\n",
        "{'left': 'light blue mug', 'right': 'orange mug'}\n",
        "{'left': 'cien bottle', 'right': 'listerine bottle'}\n",
        "{'left': 'used paper towel', 'right': 'fresh paper towel'}\n",
        "{'left': 'white cleaning cloth', 'right': 'blue cleaning cloth'}\n",
        "{'left': 'white and grey traffic post', 'right': 'white and yellow traffic post'}\n",
        "{'left': 'parking meter', 'right': 'few rocks'}\n",
        "{'left': 'tree', 'right': 'metal barrier'}\n",
        "{'left': 'slightly shorter green mailbox', 'right': 'slightly taller green mailbox'}\n",
        "{'left': 'grey recycling bin', 'right': 'green recycling bin'}\n",
        "{'left': 'metallic pole', 'right': 'blue sign'}\n",
        "{'left': 'grey traffic pole', 'right': 'red traffic cone'}\n",
        "{'left': 'green fuse box', 'right': 'fence'}\n",
        "{'left': 'wall', 'right': 'traffic cone'}\n",
        "{'left': 'intact soy milk carton', 'right': 'crushed soy milk carton'}\n",
        "{'left': 'big button', 'right': 'small button'}\n",
        "{'left': 'soap bar', 'right': 'liquid soap dispenser'}\n",
        "{'left': 'blue bottle with a red cap', 'right': 'blue bottle without a red cap'}\n",
        "{'left': 'slim fire extinguisher', 'right': 'bulky fire extinguisher'}\n",
        "{'left': 'white panel', 'right': 'fire alarm'}\n",
        "{'left': 'plant', 'right': 'can of beans'}\n",
        "{'left': 'concrete wall', 'right': 'four holes arranged in a rectangle shape'}\n",
        "{'left': 'manhole cover with the text CATV UPC', 'right': 'two packs of passata'}\n",
        "{'left': 'sidewalk', 'right': 'traffic sign'}\n",
        "{'left': 'moss', 'right': 'sidewalk'}\n",
        "{'left': 'dark blue symbol', 'right': 'light blue symbol'}\n",
        "{'left': 'pole with light blue stripes', 'right': 'pole with dark blue stripes'}\n",
        "{'left': 'orange leaves', 'right': 'green ivy leaves'}\n",
        "{'left': 'bicycle lane', 'right': 'road'}\n",
        "{'left': 'empty bag of flour', 'right': 'full bag of flour'}\n",
        "{'left': 'Three fir cones', 'right': 'Two fir cones'}\n",
        "{'left': 'guard railing', 'right': 'sidewalk'}\n",
        "{'left': 'silver posts', 'right': 'sidewalk'}\n",
        "{'left': 'wooden pole', 'right': 'tree'}\n",
        "{'left': 'most of the leaves', 'right': 'a fuse box'}\n",
        "{'left': 'metal manhole cover', 'right': 'concrete manhole cover'}\n",
        "{'left': 'sidewalk', 'right': 'street'}\n",
        "{'left': 'footwalk', 'right': 'green lamps'}\n",
        "{'left': 'green bottle with a blue head', 'right': 'green bottle with a white head'}\n",
        "{'left': 'one vertical tea spoon', 'right': 'two horizontal tea spoons'}\n",
        "{'left': 'grey chairs', 'right': 'purple chairs'}\n",
        "{'left': 'short window', 'right': 'tall window'}\n",
        "{'left': 'higher metal pole', 'right': 'metal pole'}\n",
        "{'left': 'Ivy leaves', 'right': 'tree'}\n",
        "{'left': 'a further red and white bar', 'right': 'a closer red and white bar'}\n",
        "{'left': 'green bush', 'right': 'dried plants'}\n",
        "{'left': 'gate', 'right': 'tallest tree'}\n",
        "{'left': 'image', 'right': 'peak'}\n",
        "{'left': 'sidewalk', 'right': 'road'}\n",
        "{'left': '3 glasses', 'right': '1 red berry'}\n",
        "{'left': 'five brussel sprouts', 'right': 'six red berries'}\n",
        "{'left': 'listerine bottle', 'right': 'noir bottle'}\n",
        "{'left': 'meadow fresh', 'right': 'classic houmous'}\n",
        "{'left': 'a scale on one roll of toilet paper', 'right': 'two rolls of toilet paper'}\n",
        "{'left': 'rectangular manhole cover', 'right': 'round manhole cover'}\n",
        "{'left': 'purple chair', 'right': 'grey chair'}\n",
        "{'left': 'green bush', 'right': 'dried plants'}\n",
        "{'left': 'cien bottle', 'right': 'listerine bottle'}\"\"\""
      ],
      "metadata": {
        "id": "8CyPKXx69RJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image):\n",
        "    image = image.resize((1024, 1024))\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"PNG\")\n",
        "    img_bytes = buffered.getvalue()\n",
        "    return base64.b64encode(img_bytes).decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "80GPXRwB9SRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=OPENAI_KEY, base_url=\"https://api.openai.com/v1\")\n",
        "objects_list = objects.replace(\"'\", \"\\\"\").split(\"\\n\")\n",
        "\n",
        "responses = {}\n",
        "i = 0\n",
        "for d in tqdm(dataset):\n",
        "  if d[\"Category\"] == [\"Horizontal Position\"]:\n",
        "    images = [preprocess_image(d[\"image1\"])]\n",
        "    obj = json.loads(objects_list[i])\n",
        "    prompt = \"Fill in the rough coordinates of the bounding boxes of the two objects in this json. The coordinates range from 0 to 1024 in a row-major coordinate system (also known as image coordinate system). You can do it: {\\\"\" + obj[\"right\"] + \"\\\": [xmin, ymin, xmax, ymax], \\\"\" + obj[\"left\"] + \"\\\": [xmin, ymin, xmax, ymax]} \"\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"o4-mini\", #gpt-4o\n",
        "            messages=[\n",
        "              {\"role\": \"user\", \"content\": [\n",
        "                  {\"type\": \"text\", \"text\": prompt},\n",
        "                  *[{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"}} for image in images]\n",
        "              ]}\n",
        "            ],\n",
        "            reasoning_effort=\"medium\"\n",
        "            )\n",
        "    print(response.choices[0].message.content)\n",
        "    res = response.choices[0].message.content.replace(\"\\n\", \"\")\n",
        "    match = re.search(r'\\{.*?\\}', res)\n",
        "    if match:\n",
        "        res = match.group(0)\n",
        "    else:\n",
        "        print(\"No match found\")\n",
        "        break\n",
        "\n",
        "    res = json.loads(res)\n",
        "    with open(\"output.txt\", \"a\") as file:\n",
        "        file.write(str(res[obj[\"left\"]]) + \":\" + str(res[obj[\"right\"]]) + \"\\n\")\n",
        "    responses[i] = (res[obj[\"left\"]], res[obj[\"right\"]])\n",
        "    i +=1"
      ],
      "metadata": {
        "id": "oogWhx3b9SHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QC-HmNKl9R8D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Xk_UfmrqwOTg",
        "08FYPaUDp-0f"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}